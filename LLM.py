from llama_cpp import Llama
import multiprocessing
import os
from huggingface_hub import hf_hub_download



model_path = 'model/qwen2.5-7b-instruct-q4_k_m.gguf' #  qwen2.5-7b-instruct-q4_k_m.gguf Qwen2.5-3B-Instruct-Q4_K_M.gguf
n_threads = max(1, multiprocessing.cpu_count()-2)
#n_threads = 6

system_prompt = """–¢—ã —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç, –∫–æ—Ç–æ—Ä—ã–π –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –∫–∞—Å–∞–µ–º–æ –¥—É–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏.
                    –¢–≤–æ—è –≥–ª–∞–≤–Ω–∞—è –∑–∞–¥–∞—á–∞  - –æ—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –¢–û–õ–¨–ö–û —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
                    –ï—Å–ª–∏ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –¥–ª—è –æ—Ç–≤–µ—Ç–∞, —Ç–æ —Ç—ã –æ—Ç–≤–µ—á–∞–µ—à—å : "–í –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –≤–∞—à–µ–º –∑–∞–ø—Ä–æ—Å–µ."
                    –ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π —Ñ–∞–∫—Ç—ã –¥–ª—è –æ—Ç–≤–µ—Ç–∞. –ù–ï –∏—Å–ø–æ–ª—å–∑—É–π —Å–≤–æ–∏ –∑–Ω–∞–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã—Ö –Ω–µ—Ç –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –î–∞–≤–∞–π –æ—Ç–≤–µ—Ç—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
                """

class LLMEngine():
    def __init__(self):

        self._ensure_model_exists()

        self.llm = Llama(
            model_path=model_path,
            n_ctx=2048,
            n_threads=n_threads,
            n_batch=512,
            flash_attn=True,
            n_gpu_layers=0,
            verbose=False
        )

    def _ensure_model_exists(self):

        if not os.path.exists(model_path):
            print('–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞')
            
            os.makedirs('model', exist_ok=True)

            hf_hub_download(
                repo_id="paultimothymooney/Qwen2.5-7B-Instruct-Q4_K_M-GGUF", # paultimothymooney/Qwen2.5-7B-Instruct-Q4_K_M-GGUF  bartowski/Qwen2.5-3B-Instruct-GGUF
                filename="qwen2.5-7b-instruct-q4_k_m.gguf",
                local_dir="./model",
                local_dir_use_symlinks=False)
            
            print("–ú–æ–¥–µ–ª—å —Å–∫–∞—á–µ–Ω–∞")

        else:
            print('–ú–æ–¥–µ–ª—å –Ω–∞–π–¥–µ–Ω–∞')

    def generate_response(self,  quastion, context):
        user_content = f"""
                        –ö–æ–Ω—Ç–µ–∫—Å (–ò–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ - manual.pdf):
                        {context}

                        –í–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: 
                        {quastion}
                        """
        
        response = self.llm.create_chat_completion(
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content' :user_content}
            ],
            temperature=0.1, # –≤—ã–¥–∞–µ—Ç —Å—Ç—Ä–æ–≥–∏–µ –æ—Ç–≤–µ—Ç—ã –∏–∑ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
            top_p=0.9,
            max_tokens=512,
            stop=["<|im_end|>", "–í–æ–ø—Ä–æ—Å:", "–ö–æ–Ω—Ç–µ–∫—Å—Ç:"] # —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
        )

        answer = response['choices'][0]['message']['content']
        statistic = response['usage']

        return {"answer": answer,
                 "completion_tokens": statistic["completion_tokens"],
                 "prompt_tokens": statistic["prompt_tokens"]}



# if __name__ == "__main__":
#     engine = LLMEngine()
    
#     # –≠–º—É–ª—è—Ü–∏—è RAG (–∫–∞–∫ –±—É–¥—Ç–æ –º—ã –Ω–∞—à–ª–∏ —ç—Ç–æ –≤ –±–∞–∑–µ)
#     fake_context = """
#     –†–∞–∑–¥–µ–ª: –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.
#     –¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ FCBU: 420K.
#     –≠–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ: 1.21 –ì–í—Ç.
#     """
    
#     q = "–ö–∞–∫–∞—è —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ —É FCBU?"
#     print(f"\n‚ùì –í–æ–ø—Ä–æ—Å: {q}")
#     ans = engine.generate_response(q, fake_context)
#     print(f"ü§ñ –û—Ç–≤–µ—Ç: {ans}")
