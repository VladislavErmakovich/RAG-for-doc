# RAG Chimera

**Краткое описание:** данный репозиторий представляет собой RAG-сервис на базе LLM. RAG отвечает на вопросы касаемые только документации о проекте **"Chimera"**.
Данные о самом проекте содержатся в файле **manual.pdf**. Данный сервис спроектирован под задачу, когда есть ограничение по ресурсам (8-16 GB RAM, отсутствие мощной GPU).

## Vector Data Base
Из-за ограничений в ресурсах выбор пал на **ChromaDB**. Для ее работы не надо разворачивать отдельный контейнер, как для **Qdrant** или **Milvus**, к тому же нам надо хранить информацию только из
одного документа и нет смысла поднимать для нашей задачи отдельный контейнер с базой данных (overengineering). Сама **ChromaDB** работает в режиме in-process (внутри процесса Python). При ее подключении 
использовался **chromadb.PersistentClient**, так как дынные хранятся на диске и при перезапуске данные не пропадут, а будут хранится на диске. Если бы использовался **chromadb.Client**, то при перезапуске надо было 
бы заново заполнять базу данных, так как в таком случаи данные хранятся в RAM и при отсановке работы python-файла данные бы стерлись.

## Embedding Model
Модель была выбрана **multilingual-e5-small**. Проаназилировав **MTEB Benchmark** были сделаны следующие выводы: показатель Retrivial составляет 63.19, но при этом модель занимает в памяти 449 MB, 
что на фоне других моделей это очень хорошо. Стоит отметить, что большая часть информации в файле на русском, хотя и присутсвует английский, для задачи использовалась именно мультиязычная модель.

<img width="1494" height="641" alt="image" src="https://github.com/user-attachments/assets/17fb56da-90e3-4c45-adae-a85c61d815ed" />


## LLM and Inference
**Формат модели: GGUF(llama-cpp-python)**

Был выбран такой формат в соостветствии с поставленной задачей (отсутствие мощной GPU). Форматы AWQ и GPTQ оптимизированы для GPU (CUDA ядра) и работают на CPU крайне медленно. GGUF же был разработан для CPU и 
для поставленной задачи хорошо подходит. Поэтому вкачестве inference использовалась llama-cpp-python.

**LLM: Qwen 2.5**

Данная модель была выбрана с учетом технического задания. Также эта модель хорошо подходит для русского языка. 

**Описание настроек LLM**: 

Значение temperature было выбрано 0.1, чтобы модель отвечала конкретно, по той инфромации, что ей предоставила база данных, и не вносила в ответ какой-либо инфромации (информация, на которой была обучена сама 
модель). Системный промпт выглядит следующим образом: 

*Ты технический специалист, который отвечает на вопросы касаемо дуокументации.*
*Твоя главная задача  - отвечать на вопросы ТОЛЬКО с использованием контекста. Если в контексте нет информации для ответа, то ты отвечаешь : "В документации отсутствует информация о вашем запросе." НЕ придумывай факты для ответа. НЕ используй свои знания, которых нет в контексте. Давай ответы на русском языке.*

Были проведены тесты двух моделей Qwen 2.5(3B) и Qwen 2.5(7B):

| Модель | tps | Время(c) | RAM(GB) |
| :---  | :--- | :--- | :--- |
| Qwen 2.5 (3B) | 2-7 | 12-30 | <5 |
| Qwen 2.5 (7B) | 1-4 | 30-60> | 6> |

**Тесты проходили на одних и тех же вопросах*

Стоит отметить, что большая модель хоть и отвечала долго, но ответы получались намного строже, чем у меньшей.

## Запуск проекта
1. Склонировать репзиторий через ```git clone```
2. Перейдя в папку проекта пропишите в консоли ```docker-compose up --build``` (Первый запуск может занять от 5 до 15 минут)
3. Проверка работы: зайдите в Swagger UI в браузере по http://localhost:8000/docs
4. Для останоки проекта можете нажать **Ctrl+C** или ввести команду ```docker-compose down```

## Дополнение

В репозитории отсутствуют папки: **model** и **data**. В этих папках хранятся веса для моделей и сама векторная база данных.




















